# Отчёт по лабораторной работе №1 (PostgreSQL)

## 1. Цель работы

Развернуть отдельный кластер PostgreSQL, настроить параметры сервера, создать БД, роль, табличные пространства, прикладную схему с партиционированием, наполнить данными и выполнить проверку результата.

## 2. Состав проекта

- `scripts/create.sh` — полный сценарий развертывания и проверки.
- `scripts/setup.sql` — создание БД/роли/tablespace, схемы и тестовых данных.
- `scripts/check.sql` — контрольная проверка параметров и структуры.
- `scripts/ha_pgbench_check.sh` — короткий нагрузочный тест `pgbench` с критериями `24KB` и `1500 TPS`.
- `scripts/pgbench_24kb.sql` — SQL-нагрузка: 1 транзакция = вставка payload `24KB`.
- `scripts/clean.sh` — очистка кластера и служебных директорий.

## 3. Команды выполнения

Переход в каталог лабораторной:

```bash
cd /Users/skadibtw/ddss/lab1
```

Полный запуск с очисткой:

```bash
bash scripts/create.sh --reset
```

Повторный запуск без удаления кластера:

```bash
bash scripts/create.sh
```

Отдельная проверка:

```bash
psql -v ON_ERROR_STOP=1 -p 9099 -d postgres -f scripts/check.sql
```

Проверка требования `1500 TPS` и размера транзакции `24KB`:

```bash
bash scripts/ha_pgbench_check.sh
```

Полная очистка:

```bash
bash scripts/clean.sh --yes
```

## 4. Порядок выполнения (по шагам `create.sh`)

1. `STEP 0` (опционально, `--reset`): остановка кластера и удаление директорий  
   Удаляются:
   - `$HOME/nwc36` (кластер)
   - `$HOME/sbm10` (tablespace 1)
   - `$HOME/nym69` (tablespace 2)
   - `/tmp/archive` (архив WAL)

2. `STEP 1`: подготовка директорий  
   Выполняется `mkdir -p` для директорий tablespace и архива, затем `chmod 700` для ограничения доступа.

3. `STEP 2`: `initdb` (если кластера нет)  
   Создаётся новый кластер в `$HOME/nwc36` с локалью `ru_RU.UTF-8`, кодировкой `UTF8`, checksum и параметрами аутентификации.

4. `STEP 3`: старт инстанса на порту `9099`  
   Кластер запускается через `pg_ctl`, далее готовность проверяется `pg_isready`.

5. `STEP 4`: настройка `postgresql.conf` через `ALTER SYSTEM`  
   Устанавливаются сетевые, memory, WAL, logging и locale/timezone параметры.

6. `STEP 5`: запись `pg_hba.conf`  
   Локальные подключения (`local`) — `peer`, TCP на localhost — `scram-sha-256`, внешние адреса — `reject`.

7. `STEP 6`: перезапуск кластера  
   Применение настроек, повторная проверка готовности.

8. `STEP 7`: запуск `setup.sql`  
   Создание БД, роли, tablespace, таблиц, индексов, партиций и заполнение данными.

9. `STEP 8`: запуск `check.sql`  
   Верификация параметров сервера, tablespace, распределения объектов и количества строк.

## 5. Что делает `setup.sql`

1. Включает останов при ошибке: `\set ON_ERROR_STOP on`.
2. Создаёт БД `bigbluecity` (если её нет) с `OWNER current_user`.
3. Создаёт роль `dbuser` (если её нет), задаёт пароль и срок действия.
4. Выдаёт `dbuser` доступ к БД `bigbluecity` и `postgres`.
5. Создаёт табличные пространства:
   - `sbm10_space` в `$HOME/sbm10`
   - `nym69_space` в `$HOME/nym69`
6. Выдаёт роль `dbuser` право `CREATE` на оба tablespace.
7. Подключается к `bigbluecity`, выдаёт права на `public`, переключается в `SET ROLE dbuser`.
8. Создаёт таблицы `customers`, `products`, `stores`, `sales`.
9. Настраивает RANGE-партиционирование таблицы `sales` по кварталам 2024 года.
10. Создаёт индексы с размещением в разных tablespace.
11. Загружает тестовые данные и 3000 строк в `sales`.
12. Пересоздаёт материализованное представление `sales_summary` и индекс на нём.

Примечание: сообщение  
`NOTICE: materialized view "sales_summary" does not exist, skipping`  
при первом запуске является нормальным, потому что используется `DROP MATERIALIZED VIEW IF EXISTS`.

## 6. Что проверяет `check.sql`

1. Таблица `pg_settings` для ключевых параметров:
   - `port`
   - `max_connections`
   - `shared_buffers`
   - `temp_buffers`
   - `work_mem`
   - `checkpoint_timeout`
   - `effective_cache_size`
   - `fsync`
   - `commit_delay`
   - `log_min_messages`
   - `log_connections`
   - `log_disconnections`
2. Наличие и параметры tablespace (имя, владелец, путь).
3. Размещение объектов схемы `public` по tablespace.
4. Количество строк в ключевых таблицах.
5. Фактическое распределение строк `sales` по партициям.

## 7. Обоснование выбранных параметров

### 7.1 Параметры инициализации кластера (`initdb`)

- `--encoding=UTF8`  
  Универсальная кодировка для хранения текста на любых языках, включая кириллицу. UTF8 поддерживает все Unicode-символы (>1 млн кодовых точек), что критично для русскоязычных данных и обеспечивает совместимость с современными приложениями. Альтернативы типа `WIN1251` или `KOI8-R` устарели и ограничены набором символов.

- `--locale=ru_RU.UTF-8` и `--lc-* ru_RU.UTF-8`  
  Локаль определяет правила сортировки (collation), форматирование дат, чисел и денежных сумм, а также язык системных сообщений. Указание `ru_RU.UTF-8` для всех категорий (`LC_COLLATE`, `LC_CTYPE`, `LC_MESSAGES`, `LC_MONETARY`, `LC_NUMERIC`, `LC_TIME`) обеспечивает:
  - Корректную сортировку русских строк (Ё между Е и Ж, а не в конце алфавита);
  - Формат даты `ДД.ММ.ГГГГ` и времени `ЧЧ:ММ:СС`;
  - Разделитель тысяч и десятичного знака по российским стандартам;
  - Сообщения об ошибках на русском языке.
  
  Единая локаль упрощает поддержку и предотвращает несовместимость при миграции данных.

- `--data-checksums`  
  Включает вычисление CRC32 для каждой 8KB страницы данных и запись контрольной суммы в заголовок страницы. При чтении страницы PostgreSQL проверяет checksum и обнаруживает повреждения, вызванные сбоями диска, битовой коррупцией в RAM или ошибками файловой системы. Включение увеличивает CPU-нагрузку на ~1-2% при записи, но критично для раннего обнаружения проблем (до потери данных или каскадных сбоев). Параметр задается только при инициализации и не может быть изменен позже без пересоздания кластера.

- `--auth=peer`  
  Аутентификация для локальных (Unix socket) подключений по UID пользователя ОС. Ядро ОС передает PostgreSQL PID и UID подключающегося процесса, сервер проверяет соответствие имени пользователя ОС имени роли в БД. Это безопаснее пароля (нет хранения/передачи credentials) и удобнее для администрирования (`psql -d postgres` без параметров подключает под текущим пользователем). Работает только на Linux/UNIX через Unix domain sockets.

- `--auth-host=scram-sha-256`  
  Метод аутентификации для TCP-подключений. SCRAM-SHA-256 — современный стандарт (RFC 7677), заменивший небезопасный MD5:
  - Пароль не передается по сети (используется challenge-response);
  - Защита от rainbow table и replay-атак;
  - Криптографически стойкое хэширование (SHA-256 + соль).
  
  Альтернативы: `md5` (deprecated, уязвим), `password` (передача пароля открытым текстом), `trust` (без проверки пароля) — все небезопасны для production. SCRAM поддерживается клиентами начиная с PostgreSQL 10+ и всеми современными драйверами.

### 7.2 Сетевые параметры

- `port = 9099`  
  Нестандартный порт выбран для изоляции от дефолтного `5432` и предотвращения конфликтов с уже запущенными инстансами PostgreSQL на том же хосте. Это типичная практика для учебных/тестовых кластеров, когда на одной машине может работать несколько версий или инстансов PostgreSQL. Порт из диапазона 9000-9999 не конфликтует со стандартными сервисами и легко запоминается.

- `listen_addresses = 'localhost'`  
  Ограничение прослушивания только локальными интерфейсами (127.0.0.1 и ::1) минимизирует поверхность атаки: сервер недоступен из внешней сети даже при открытом порте в firewall. Для учебной работы внешний доступ не требуется. В production при необходимости удаленного доступа используется конкретный IP-адрес или `*` (все интерфейсы) в сочетании с жесткими правилами `pg_hba.conf`.

- `unix_socket_directories = '/tmp'`  
  Расположение Unix domain socket — наиболее быстрый способ подключения для локальных клиентов (без сетевого стека). Каталог `/tmp` общепринят и доступен по умолчанию. Альтернативы (`/var/run/postgresql`) могут требовать дополнительных прав. Socket-подключения используют аутентификацию `peer` (по UID процесса), что безопасно и удобно для администрирования.

### 7.3 Память и производительность

- `max_connections = 300`  
  Значение выбрано для демонстрации многопользовательского режима с учетом требования `1500 TPS`. При среднем времени транзакции ~10-20мс и `pgbench` с 100 клиентами, запас в 300 соединений обеспечивает комфортную работу без исчерпания лимита. Каждое соединение потребляет ~400 байт в `shared_buffers` плюс выделенную память для запросов (`work_mem`), поэтому значение 300 балансирует между многопользовательским режимом и ограничениями памяти.

- `shared_buffers = 1GB`  
  Рекомендуемое значение для выделенного сервера БД — 25% от RAM (при RAM 4-8GB). 1GB обеспечивает кэширование горячих данных в памяти PostgreSQL, снижая число обращений к диску. Большее значение (>40% RAM) может конфликтовать с кэшем ОС и ухудшить производительность. Меньшее значение (<512MB) приведет к частым дисковым операциям.

- `temp_buffers = 16MB`  
  Используется для временных таблиц внутри сессии. Значение 16MB достаточно для большинства операций с временными данными без избыточного выделения памяти на каждое из 300 потенциальных соединений (16MB × 300 = 4.8GB максимум).

- `work_mem = 4MB`  
  Память для операций сортировки и хэш-таблиц в рамках одного запроса. При 300 соединениях и потенциально нескольких операциях сортировки на запрос, максимальное потребление может достичь 300 × 4MB × N операций. Значение 4MB — безопасный компромисс: достаточно для небольших сортировок в памяти, но не приведет к исчерпанию RAM при множественных параллельных запросах. Для сложных аналитических запросов можно временно увеличить через `SET work_mem`.

- `checkpoint_timeout = 10min`  
  Стандартное значение 5 минут создает частые пики I/O. Увеличение до 10 минут снижает частоту checkpoint вдвое, распределяя нагрузку на диск более равномерно. При объеме WAL 1-4GB это оптимально для тестовой нагрузки. Слишком большое значение (>30 мин) увеличит время восстановления после сбоя.

- `effective_cache_size = 3GB`  
  Оценка доступной памяти для кэширования данных (PostgreSQL + кэш ОС). При RAM 4-8GB и `shared_buffers=1GB`, остается ~3GB для файлового кэша ОС. Это помогает планировщику запросов выбирать между index scan и sequential scan: большее значение склоняет к использованию индексов.

- `fsync = on`  
  Критично для ACID-гарантий. Отключение (`fsync=off`) ускоряет запись, но риск потери данных при сбое питания неприемлем для production. Значение `on` обязательно для надежности.

- `commit_delay = 10`, `commit_siblings = 5`  
  Микрооптимизация для высоконагруженных систем: если одновременно коммитятся >=5 транзакций, задержка 10 микросекунд позволяет сгруппировать запись WAL, снижая количество `fsync` вызовов. При требовании 1500 TPS (множество параллельных транзакций) это дает 5-10% выигрыш в пропускной способности без заметного увеличения latency.

### 7.4 WAL и архивирование

- `wal_level = replica`  
  Минимальный уровень для поддержки репликации и point-in-time recovery (PITR). Уровень `minimal` не подходит для архивирования, а `logical` избыточен (используется только для логической репликации). Значение `replica` — оптимальный баланс между функциональностью и объемом записываемых WAL.

- `synchronous_commit = on`  
  Гарантирует, что `COMMIT` возвращает успех только после физической записи WAL на диск. Отключение (`off` или `local`) ускоряет транзакции на 20-30%, но создает окно риска потери последних транзакций при сбое. Для требования надежности в лабораторной работе выбрано `on`.

- `min_wal_size = 1GB`, `max_wal_size = 4GB`  
  PostgreSQL сохраняет WAL-сегменты между checkpoint для переиспользования. `min_wal_size=1GB` предотвращает частое создание/удаление файлов при низкой нагрузке. `max_wal_size=4GB` ограничивает рост WAL и частоту checkpoint: при превышении этого порога принудительный checkpoint запускается раньше `checkpoint_timeout`. Разница в 4 раза (1GB-4GB) обеспечивает плавное масштабирование под нагрузку без чрезмерного потребления дискового пространства.

- `archive_mode = on`  
  Активирует выполнение `archive_command` для каждого заполненного WAL-сегмента. Необходимо для создания базового резервного копирования и PITR. Без этого WAL-сегменты удаляются без архивирования.

- `archive_command = 'test ! -f /tmp/archive/%f && cp %p /tmp/archive/%f'`  
  Простейшая команда для учебной работы: копирует WAL-сегмент в `/tmp/archive/`, предварительно проверяя отсутствие файла с таким именем (защита от перезаписи). В production используются более сложные схемы (сжатие, удаленное хранилище, верификация), но для демонстрации концепта достаточно копирования. Команда возвращает ненулевой exit code при ошибке, что блокирует удаление WAL до успешного архивирования.

- `archive_timeout = 300`  
  Принудительное переключение WAL-сегмента каждые 5 минут, даже если он не заполнен. Это ограничивает максимальную потерю данных при восстановлении из архива (RPO ≤ 5 минут). Без этого параметра при низкой активности один сегмент (обычно 16MB) может оставаться открытым часами. Значение 300 секунд — разумный компромисс между актуальностью архива и частотой операций переключения.

### 7.5 Логирование

- `logging_collector = on`, `log_destination = 'stderr'`, `log_directory = 'log'`  
  Централизованный сбор логов через фоновый процесс logger. `log_destination='stderr'` направляет сообщения в стандартный поток ошибок, который logger перехватывает и записывает в файлы в `$PGDATA/log/`. Это надежнее, чем логирование через syslog (зависимость от внешнего сервиса) или csvlog (избыточность формата для учебной работы). Отдельная директория `log/` упрощает ротацию и анализ.

- `log_filename = 'postgresql-%Y-%m-%d_%H%M%S.log'`  
  Ротация логов по времени с точностью до секунды. Формат с датой в имени файла позволяет легко находить записи за конкретный период и предотвращает перезапись при быстрых перезапусках сервера. Штампы времени (`%Y-%m-%d_%H%M%S`) стандартны и поддерживают сортировку по имени файла.

- `log_min_messages = 'notice'`  
  Уровень детализации логирования. `notice` включает важные сообщения (WARNING, ERROR, FATAL, PANIC) плюс уведомления (например, о создании индексов, выполнении VACUUM). Более низкие уровни (`info`, `debug`) создают избыточный объем логов при нагрузке. Более высокие (`warning`) скрывают полезную диагностическую информацию. Для учебной работы `notice` — оптимальный баланс: достаточно информативен, но не перегружает логи.

- `log_connections = on`, `log_disconnections = on`  
  Фиксация каждого подключения/отключения клиента с указанием пользователя, БД и IP-адреса. Это критично для аудита безопасности (кто и когда подключался), диагностики проблем с соединениями (connection leaks, таймауты) и анализа нагрузки (количество сессий в единицу времени). Накладные расходы минимальны (~1 строка лога на соединение).

- `log_line_prefix = '%m [%p] user=%u db=%d app=%a client=%h '`  
  Префикс каждой строки лога содержит контекст:
  - `%m` — timestamp с миллисекундами (для корреляции событий);
  - `%p` — PID процесса backend (для группировки сообщений одной сессии);
  - `%u` — имя пользователя БД;
  - `%d` — имя базы данных;
  - `%a` — имя приложения (задается клиентом);
  - `%h` — IP-адрес/hostname клиента.
  
  Этот набор позволяет быстро фильтровать логи по пользователю, БД или клиенту без парсинга текста сообщений. Альтернативные варианты (например, добавление `%l` — номера строки лога, `%v` — virtual transaction ID) создают избыточность для базовой диагностики.

### 7.6 Безопасность доступа (`pg_hba.conf`)

- `local all all peer`  
  Правило для Unix socket соединений (`local`): любой пользователь ОС (`all`) может подключиться к любой БД (`all`) под ролью с совпадающим именем. Метод `peer` проверяет соответствие через UID процесса (см. раздел 7.1). Это правило размещено первым, чтобы иметь приоритет: локальные администраторы получают доступ без пароля, что упрощает восстановление при потере credentials.

- `host all all 127.0.0.1/32 scram-sha-256` и `::1/128`  
  TCP-подключения к любой БД с локального хоста (IPv4 `127.0.0.1/32` и IPv6 `::1/128`) требуют SCRAM-SHA-256 аутентификации. Клиент должен предоставить корректную пару логин/пароль. Это правило защищает от несанкционированного доступа через сеть даже на локальной машине (например, от процессов под другим UID или при использовании TCP вместо Unix socket). Маска `/32` и `/128` означает точное совпадение адреса (только loopback-интерфейс).

- `host all all 0.0.0.0/0 reject` и `::/0 reject`  
  Явный запрет подключений с любых внешних адресов (IPv4 `0.0.0.0/0` — все адреса IPv4, IPv6 `::/0` — все адреса IPv6). Метод `reject` немедленно разрывает соединение с сообщением об отказе, не тратя ресурсы на аутентификацию. Эти правила размещены последними и действуют как fallback: если адрес не совпал с предыдущими правилами (localhost), доступ блокируется.

Порядок правил критичен: `pg_hba.conf` обрабатывается сверху вниз, применяется первое совпадающее правило. Конфигурация реализует "белый список" (разрешен только localhost) вместо "черного списка", что соответствует best practice безопасности. При необходимости удаленного доступа добавляются конкретные trusted IP-адреса с `scram-sha-256` между правилами localhost и reject.

## 8. Проверка 1500 TPS и 24KB на транзакцию (`pgbench`)

Для отдельной проверки добавлен сценарий:

```bash
bash scripts/ha_pgbench_check.sh
```

Что делает скрипт:

1. Подключается к БД `bigbluecity` на порту `9099`.
2. Создаёт/очищает таблицу `bench_24kb`.
3. Запускает `pgbench` с custom SQL `scripts/pgbench_24kb.sql`:
   - `-c $CLIENTS -j $JOBS -T $DURATION -M prepared -r -P 1`;
   - значения по умолчанию: `CLIENTS=100`, `JOBS=4`, `DURATION=600`;
   - одна транзакция выполняет `INSERT` payload ровно `24576` байт (`24KB`).
4. Извлекает итоговый TPS из вывода `pgbench`.
5. Проверяет фактический размер данных:
   - `MIN(octet_length(payload)) = 24576`;
   - `MAX(octet_length(payload)) = 24576`.
6. Сравнивает итоговый TPS с порогом `1500`.

Критерий успешного прохождения:

- скрипт завершился без ошибки (`exit code 0`);
- в выводе присутствует строка:
  - `PASS: TPS=<значение>, tx_size=24KB`;
- одновременно истинны два условия:
  - `TPS >= 1500`;
  - фактический размер каждой транзакционной записи равен `24576` байт.

Ограничение:

- данный сценарий не включает настройку репликации и failover (проверяется только нагрузка и размер транзакции).
- при ограничениях ОС по числу процессов/подключений возможна ошибка `Resource temporarily unavailable`; в этом случае уменьшаются `CLIENTS` и/или `JOBS` до стабильного уровня, при сохранении критерия `TPS >= 1500`.

### 8.1 Целевые метрики производительности

Приведенные ниже метрики являются **ориентировочными** и зависят от конкретного оборудования:

| Параметр | Целевое значение | Обоснование |
|----------|------------------|-------------|
| **TPS** (без задержек) | **≥ 1400-1500** | Достижимо при NVMe SSD с учетом `synchronous_commit=on` и группировки commits. При 100 клиентах и latency ~15 мс: `TPS = 100 / 0.015 ≈ 6666` (теоретически), но реально ограничено fsync (~1500-3000 TPS). При 24KB/tx это **36 MB/s** запись данных + WAL. |
| **Avg Latency** | **≤ 15 мс** | Складывается из: Parse/Plan (~0.5 мс) + Execute INSERT (~2-3 мс) + WAL write (~0.5-1 мс) + **fsync (~8-12 мс)** + Network (~0.1 мс). Основной вклад — `synchronous_commit=on` (ожидание записи на диск). С `commit_delay=10` группировка снижает среднюю latency до 10-15 мс. |
| **95th Percentile Latency** | **≤ 30 мс** | В 2 раза больше средней — нормально для стабильной системы. Выбросы вызваны checkpoint spikes (каждые 10 мин), WAL rotation, lock contention при 100 параллельных клиентах. Если p95 > 50 мс — признак проблем (недостаток IOPS, memory pressure). |
| **Buffer Hit Rate** | **≥ 99%** | Процент страниц данных из `shared_buffers` без disk read. При 1GB (~131K страниц) и ограниченном working set (индексы + hot data) достижимо 99%. Остальной 1% — cold reads, расширение файлов, checkpoint reads. Если < 98% — увеличить `shared_buffers`. |

#### Почему значения могут отличаться:

**Факторы, влияющие на результаты:**

1. **Тип диска:**
   - **NVMe SSD**: 1400-1500 TPS ✓
   - **SATA SSD**: 800-1200 TPS (fsync медленнее в 1.5-2 раза)
   - **HDD**: 100-300 TPS (fsync ~10-15 мс → latency 30-50 мс)

2. **Оперативная память:**
   - **4-8 GB RAM**: buffer hit rate 99% при текущем workload
   - **< 4 GB**: hit rate падает до 95-97% → больше disk I/O → ниже TPS
   - **> 8 GB**: можно увеличить `shared_buffers` до 2GB → выше hit rate

3. **CPU:**
   - **4+ ядра**: 100 клиентов обрабатываются параллельно
   - **1-2 ядра**: context switching → latency +5-10 мс → TPS падает до 1000-1200

4. **Операционная система:**
   - **Linux (ext4/xfs)**: оптимальная производительность
   - **macOS (APFS)**: fsync медленнее на 20-30%
   - **Windows (NTFS)**: fsync медленнее на 30-50%, TPS может быть 1000-1200

5. **Фоновая нагрузка:**
   - Другие процессы на сервере → меньше IOPS для PostgreSQL
   - Антивирусы, индексация диска → latency spikes

#### Интерпретация результатов:

**Если TPS < 1400:**
- Проверить тип диска (`iostat -x 1` или `perfmon` в Windows)
- Попробовать `synchronous_commit=local` (latency ~3-5 мс, TPS 2000-3000)
- Увеличить `commit_delay` до 50-100 (больше группировка)

**Если Avg Latency > 20 мс:**
- Проверить disk write latency (медленный диск?)
- Временно отключить `fsync=off` для теста (latency падает до 2-3 мс)
- Добавить RAM или увеличить `shared_buffers`

**Если p95 > 50 мс:**
- Настроить checkpoint: `checkpoint_completion_target=0.9` (растянуть запись)
- Увеличить `max_wal_size` до 8GB (реже checkpoint)
- Проверить `pg_stat_bgwriter` (слишком частые checkpoints?)

**Если Buffer Hit Rate < 98%:**
- Увеличить `shared_buffers` до 2GB (если RAM позволяет)
- Проверить `pg_stat_statements`: есть ли sequential scans вместо index scans?
- Возможно, working set больше доступной памяти

**Примечание:** Для учебной работы достижение порядка величин (TPS ~1000-1500, latency ~10-20 мс) важнее точного соответствия целевым значениям. Основная цель — продемонстрировать понимание влияния параметров конфигурации на производительность.

## 9. Итог

В результате работы развернут и настроен изолированный кластер PostgreSQL на порту `9099`, создана БД `bigbluecity`, роль `dbuser`, два табличных пространства, партиционированная схема и тестовые данные. Настройки сервера и структура подтверждаются скриптом `check.sql`.
